{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t \n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from torchvision import datasets, transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' # cuda or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "init_log_sigma = -3.0 # логарифм дисперсии вариационного распределения при инициализации\n",
    "prior_sigma = 0.1 # априорная дисперсия\n",
    "epoch_num = 2 #количество эпох\n",
    "lamb = [0, 0.1, 0.5, 1, 5, 10, 100, 1000]\n",
    "# lam = 1.0 # коэффициент перед дивергенцией\n",
    "hidden_num = 100 # количество нейронов на скрытом слое\n",
    "t.manual_seed(42) # задаем значение генератора случайных чисел для повторяемости экспериментов\n",
    "acc_delete = [] \n",
    "filename = 'Hypernet_linear_1_sn3' # куда сохранять\n",
    "lam_hidden_num = 1\n",
    "start_num = 1\n",
    "log_lam_low = -2.0\n",
    "log_lam_high = 2.0\n",
    "mode = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение данных\n",
    "def save(file):\n",
    "    outfile = open(filename, 'wb')\n",
    "    pickle.dump(file, outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "def load(path = filename):\n",
    "    infile = open(path, 'rb')\n",
    "    file = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return file\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка данных\n",
    "train_data = torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                             transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                              ]))\n",
    "\n",
    "test_data = torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                             transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
    "                              ]))\n",
    "\n",
    "\n",
    "train_loader = t.utils.data.DataLoader(train_data, batch_size=batch_size, pin_memory=True )\n",
    "test_loader = t.utils.data.DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankNet(nn.Module):\n",
    "    def __init__(self, size, hidden, gain_const = 1.0, gain_lamb = 1.0,\n",
    "                 gain_lowrank = .0001,  act= F.relu):    \n",
    "        nn.Module.__init__(self)        \n",
    "        self.w = nn.Linear(1, hidden).to(device)\n",
    "        t.nn.init.xavier_uniform(self.w.weight, gain_lamb)\n",
    "        # проверка на вектор или матрица\n",
    "        if isinstance(size, tuple) and len(size) == 2: # если сайз неизменяемый список и его длина 2\n",
    "            self.in_, self.out_ = size\n",
    "            self.diagonal = False\n",
    "        else:\n",
    "            self.out_ = size\n",
    "            self.diagonal = True\n",
    "            \n",
    "        \n",
    "        self.one = t.ones(1,device=device) # для упрощения работы с лямбдой. Костыль, можно сделать проще\n",
    "        self.act = act\n",
    "        \n",
    "        if self.diagonal:\n",
    "            self.w_d = nn.Linear(hidden, self.out_).to(device)\n",
    "            t.nn.init.xavier_uniform(self.w_d.weight, gain_lowrank)\n",
    "            # независимая от параметра lambda часть\n",
    "            self.const = nn.Parameter(t.randn(self.out_, device=device)) \n",
    "            \n",
    "        else:\n",
    "            self.w_a1 = nn.Linear(hidden, self.in_).to(device)\n",
    "            t.nn.init.xavier_uniform(self.w_a1.weight, gain_lowrank)\n",
    "            \n",
    "            self.w_a2 = nn.Linear(hidden, self.out_).to(device)\n",
    "            t.nn.init.xavier_uniform(self.w_a2.weight, gain_lowrank)\n",
    "            \n",
    "            self.const = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform(self.const,  gain_const)\n",
    "            \n",
    "            \n",
    "    def forward(self, lam):\n",
    "        h = self.act(self.w(self.one * lam))        \n",
    "        if self.diagonal:\n",
    "            return self.const + self.w_d(h)\n",
    "        else:\n",
    "            a1 = self.w_a1(h)\n",
    "            a2 = self.w_a2(h)\n",
    "         \n",
    "            return self.const +  t.matmul(a1.view(-1, 1), a2.view(1, -1))\n",
    "\n",
    "        \n",
    "class LinearApprNet(nn.Module):\n",
    "    def __init__(self, size,  gain_const = 1.0, gain_const2 = 0.000001,  act= lambda x: x):    \n",
    "        nn.Module.__init__(self)        \n",
    "        if isinstance(size, tuple) and len(size) == 2:\n",
    "            self.in_, self.out_ = size\n",
    "            self.diagonal = False\n",
    "        else:\n",
    "            self.out_ = size\n",
    "            self.diagonal = True\n",
    "            \n",
    "        \n",
    "        self.one = t.ones(1, device=device) # для упрощения работы с лямбдой. Костыль, можно сделать проще\n",
    "        self.act = act\n",
    "        \n",
    "        if self.diagonal:\n",
    "            # независимая от параметра lambda часть\n",
    "            self.const = nn.Parameter(t.randn(self.out_, device=device)) \n",
    "            self.const2 = nn.Parameter(t.ones(self.out_, device=device) * gain_const2) \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            self.const = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform(self.const,  gain_const)\n",
    "            self.const2 = nn.Parameter(t.randn(self.in_, self.out_, device=device)) \n",
    "            t.nn.init.xavier_uniform(self.const2,  gain_const2)\n",
    "            \n",
    "            \n",
    "    def forward(self, lam):        \n",
    "        if self.diagonal:\n",
    "            return self.const + self.const2 * lam\n",
    "        else:\n",
    "            return self.const + self.const2 * lam \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:21: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1579e-03,  4.4358e-04, -3.1865e-04,  3.2437e-04,  9.0420e-05,\n",
       "         3.2893e-04,  4.0364e-04,  8.0103e-04,  3.8385e-04, -1.4532e-04,\n",
       "         9.3460e-05, -8.5342e-04, -4.9203e-04,  1.0955e-03,  2.6686e-04,\n",
       "        -5.0107e-04, -6.9356e-04, -3.5524e-05,  1.4722e-04, -4.9192e-04,\n",
       "         1.3196e-04,  7.8094e-04, -5.0154e-04,  3.7491e-04, -7.2670e-04,\n",
       "        -4.2802e-04, -2.1482e-04, -6.1572e-05,  7.6455e-04, -8.5819e-04,\n",
       "        -5.1409e-04, -3.8001e-04, -2.3258e-04, -5.6469e-04, -4.0627e-04,\n",
       "        -3.2903e-04,  1.0856e-03, -8.3484e-04, -9.5175e-04,  6.8972e-04,\n",
       "         1.2767e-04, -1.1337e-03,  3.4571e-04,  6.0070e-04,  5.2279e-04,\n",
       "        -3.9132e-04, -1.2994e-04, -1.7625e-04,  7.2122e-06, -2.2772e-04,\n",
       "        -2.9835e-04, -1.7881e-07,  6.0558e-04,  4.9710e-05,  6.5863e-04,\n",
       "         7.4020e-04,  2.1636e-04,  2.3735e-04,  4.2582e-04,  1.0834e-03,\n",
       "        -4.0892e-04, -7.6619e-04, -2.2078e-04,  4.9284e-04, -5.7805e-04,\n",
       "         9.3067e-04,  1.1504e-04,  5.8293e-04, -4.3842e-04, -2.7531e-04,\n",
       "         7.2092e-04,  3.7593e-04,  2.9117e-04,  2.3484e-05, -5.7387e-04,\n",
       "        -3.7020e-04,  8.8215e-06,  7.8678e-05,  4.6432e-04,  8.3748e-04,\n",
       "        -7.3135e-05, -6.2662e-04, -8.1255e-04,  3.6323e-04,  4.7088e-04,\n",
       "         6.2281e-04, -6.8094e-04,  7.3740e-04,  6.7186e-04, -8.2117e-04,\n",
       "         6.0511e-04,  3.1531e-05,  7.1287e-05,  9.3099e-04, -1.8239e-05,\n",
       "        -8.8847e-04,  4.2440e-04,  3.1684e-04, -3.1102e-04, -4.4066e-04],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка что все работает\n",
    "# случай вектора\n",
    "n = LowRankNet(100, 10)\n",
    "n(100)- n(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:27: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:30: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-6.1625e-04,  4.0583e-04, -6.5077e-05,  ...,  3.8743e-07,\n",
       "          3.6196e-04,  3.4638e-04],\n",
       "        [ 6.1025e-04, -4.2704e-04, -5.9105e-05,  ..., -2.4839e-04,\n",
       "         -5.2603e-04, -4.3452e-05],\n",
       "        [-5.3713e-04,  3.5816e-04, -3.5007e-05,  ...,  4.3936e-05,\n",
       "          3.4495e-04,  2.4928e-04],\n",
       "        ...,\n",
       "        [-6.3423e-04,  4.0828e-04, -1.1307e-04,  ..., -9.2089e-05,\n",
       "          3.1003e-04,  4.6821e-04],\n",
       "        [ 6.0985e-04, -4.0704e-04,  3.7774e-05,  ..., -5.3838e-05,\n",
       "         -3.9433e-04, -2.7824e-04],\n",
       "        [ 8.3520e-04, -5.5788e-04,  4.9610e-05,  ..., -7.7993e-05,\n",
       "         -5.4292e-04, -3.7590e-04]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка что все работает\n",
    "# случай матрицы\n",
    "n = LowRankNet((100, 20), 10)\n",
    "n(100) - n(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarLayer(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = nn.Parameter(t.randn(in_, out_, device=device)) # параметры средних\n",
    "        t.nn.init.xavier_uniform(self.mean) \n",
    "        self.log_sigma = nn.Parameter(t.ones(in_, out_, device = device)*init_log_sigma) # логарифм дисперсии\n",
    "        self.mean_b = nn.Parameter(t.randn(out_, device=device)) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = nn.Parameter(t.ones(out_, device=device) * init_log_sigma)\n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            self.eps_w = t.distributions.Normal(self.mean, t.exp(self.log_sigma))\n",
    "            self.eps_b = t.distributions.Normal(self.mean_b, t.exp(self.log_sigma_b))\n",
    "        \n",
    "            w = self.eps_w.rsample()\n",
    "            b = self.eps_b.rsample()\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean \n",
    "            b = self.mean_b\n",
    "            \n",
    "        # функция активации \n",
    "        return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean, t.exp(self.log_sigma))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b,  t.exp(self.log_sigma_b))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2\n",
    "    \n",
    "class VarLayerLowRank(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = LowRankNet((in_, out_), lam_hidden_num) # параметры средних            \n",
    "        self.log_sigma = LowRankNet((in_, out_), lam_hidden_num) # логарифм дисперсии\n",
    "        self.mean_b = LowRankNet( out_, lam_hidden_num) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = LowRankNet( out_, lam_hidden_num)\n",
    "     \n",
    "        self.log_sigma.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma.const.data+= init_log_sigma\n",
    "     \n",
    "        self.log_sigma_b.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma_b.const.data+= init_log_sigma\n",
    "        \n",
    "        \n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x, l):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "            self.eps_b = t.distributions.Normal(self.mean_b(l), t.exp(self.log_sigma_b(l)))\n",
    "        \n",
    "            w = self.eps_w.rsample()\n",
    "            b = self.eps_b.rsample()\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean(l) \n",
    "            b = self.mean_b(l)\n",
    "            \n",
    "        # функция активации \n",
    "        return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self, l):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b(l),  t.exp(self.log_sigma_b(l)))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2\n",
    "    \n",
    "\n",
    "class VarLayerLinearAppr(nn.Module): # вариационная однослойная сеть\n",
    "    def __init__(self, in_,  out_,   act=F.relu):         \n",
    "        nn.Module.__init__(self)                    \n",
    "        self.mean = LinearApprNet((in_, out_)) # параметры средних            \n",
    "        self.log_sigma = LinearApprNet((in_, out_)) # логарифм дисперсии\n",
    "        self.mean_b = LinearApprNet( out_) # то же самое для свободного коэффициента\n",
    "        self.log_sigma_b = LinearApprNet( out_)\n",
    "     \n",
    "        self.log_sigma.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma.const.data+= init_log_sigma\n",
    "     \n",
    "        self.log_sigma_b.const.data*= 0 # забьем константу нужными нам значениями\n",
    "        self.log_sigma_b.const.data+= init_log_sigma\n",
    "        \n",
    "        \n",
    "                \n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        if self.training: # во время обучения - сэмплируем из нормального распределения\n",
    "            self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "            self.eps_b = t.distributions.Normal(self.mean_b(l), t.exp(self.log_sigma_b(l)))\n",
    "        \n",
    "            w = self.eps_w.rsample()\n",
    "            b = self.eps_b.rsample()\n",
    "             \n",
    "        else:  # во время контроля - смотрим средние значения параметра        \n",
    "            w = self.mean(l) \n",
    "            b = self.mean_b(l)\n",
    "            \n",
    "        # функция активации \n",
    "        return self.act(t.matmul(x, w)+b)\n",
    "\n",
    "    def KLD(self, l):        \n",
    "        # подсчет дивергенции\n",
    "        size = self.in_, self.out_\n",
    "        out = self.out_\n",
    "        self.eps_w = t.distributions.Normal(self.mean(l), t.exp(self.log_sigma(l)))\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b(l),  t.exp(self.log_sigma_b(l)))\n",
    "        self.h_w = t.distributions.Normal(t.zeros(size, device=device), t.ones(size, device=device)*prior_sigma)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(out, device=device), t.ones(out, device=device)*prior_sigma)                \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:69: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:71: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "l = VarLayerLinearAppr(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9999, -2.9999, -2.9999, -2.9999, -2.9999, -2.9999, -2.9999, -2.9999,\n",
       "        -2.9999, -2.9999], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.log_sigma_b(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarSeqNet(nn.Sequential):    \n",
    "    # класс-обертка на случай, если у нас многослойная нейронная сеть\n",
    "    def KLD(self, lam = None):\n",
    "        k = 0\n",
    "        for l in self: \n",
    "            if lam is None:\n",
    "                k+=l.KLD()\n",
    "            else:\n",
    "                k+=l.KLD(lam)\n",
    "                \n",
    "        return k\n",
    "    \n",
    "    def forward(self, x, lam = None):\n",
    "        if lam is None:\n",
    "            for l in self:\n",
    "                x = l(x)\n",
    "            return x\n",
    "        else:\n",
    "            for l in self:\n",
    "                x = l(x, lam)\n",
    "            return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches(net, loss_fn, optimizer, i, out, out_loss, kld, loss, epoch):\n",
    "    for id, (x,y) in enumerate(train_loader):  \n",
    "            id+=1\n",
    "            if device == 'cuda':\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()            \n",
    "            optimizer.zero_grad() \n",
    "            loss[i] = 0 \n",
    "            #for _ in range(5):\n",
    "            log_lam = np.random.uniform(low=log_lam_low, high=log_lam_high)\n",
    "            lam = 10**log_lam \n",
    "            #lam = 1.0\n",
    "            lam_param = lam/10**(log_lam_high) # нормируем вход\n",
    "            out[i] = net(x, lam_param)\n",
    "            # правдоподобие должно суммироваться по всей обучающей выборке\n",
    "            # в случае батчей - она приводится к тому же порядку \n",
    "            out_loss[i] = loss_fn(out[i], y)* len(train_data)                \n",
    "            kld[i] =  net.KLD(lam_param) *lam\n",
    "        \n",
    "\n",
    "\n",
    "            loss[i] += (out_loss[i]+kld[i])       \n",
    "            if id %100 == 0:           \n",
    "                print (\"Number of net:\",i, loss[i].data, out_loss[i].data, kld[i].data, lam)            \n",
    "                    \n",
    "            loss[i].backward()       \n",
    "            clip_grad_value_(net.parameters(), 1.0) # для стабильности градиента. С этим можно играться\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(net, loss_fn, i, kld, loss, out, out_loss):\n",
    "    net.eval()  \n",
    "    kld[i] =  net.KLD(1) \n",
    "    loss[i] = kld[i]\n",
    "    for x,y in test_loader:\n",
    "         if device == 'cuda':\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()          \n",
    "    out[i] = net(x, 1)   \n",
    "    out_loss[i] = loss_fn(out[i], y)* len(train_data)\n",
    "    #  print(out_loss[i])\n",
    "    # print(loss[i])\n",
    "    loss[i] += out_loss[i]\n",
    "    net.train()\n",
    "    print (loss[i])\n",
    "    return loss[i]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# рассмотрим для примера сеть, состояющую из двух слоев\n",
    "# второй слой - softmax. По сути для обучения задавать активацию явно не нужно, она забита в nn.CrossEntropyLoss\n",
    "def init_nets(loss_fn_nets):\n",
    "    for i in range(start_num):\n",
    "        if mode == 'lowrank':\n",
    "            nets.append(VarSeqNet(VarLayerLowRank(784,  hidden_num), VarLayerLowRank(hidden_num, 10, act=lambda x:x)))\n",
    "        elif mode == 'linear':\n",
    "            nets.append(VarSeqNet(VarLayerLinearAppr(784,  hidden_num), VarLayerLinearAppr(hidden_num, 10, act=lambda x:x)))\n",
    "        else:\n",
    "            raise ValueError('Bad mode')\n",
    "        optimizer_nets.append(optim.Adam(nets[i].parameters(), lr=0.001))\n",
    "        loss_fn_nets.append(nn.CrossEntropyLoss())\n",
    "    loss_graph=[[],[],[]]\n",
    "    out = [None, None, None]\n",
    "    out_loss = [None, None, None]\n",
    "    kld = [None, None, None]\n",
    "    loss = [None, None, None]\n",
    "    return out, out_loss, kld, loss, loss_graph\n",
    "\n",
    "def train_nets(out, out_loss, kld, loss, loss_graph):\n",
    "    for epoch in range(epoch_num):             \n",
    "        for i,net in enumerate(nets):\n",
    "            train_batches(net,loss_fn_nets[i], optimizer_nets[i],i, out, out_loss, kld, loss, epoch)\n",
    "        print ('end of epoch: ', epoch)   \n",
    "        for i,net in enumerate(nets):\n",
    "            print(\"Number of net:\",i)        \n",
    "            loss_graph[i].append(statistic(net, loss_fn_nets[i], i, kld, loss, out, out_loss))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(loss_graf)\n",
    "def graph_loss_func(loss_graph, nets):\n",
    "    for i,net in enumerate(nets): \n",
    "        plt.plot(loss_graph[i])\n",
    "    plt.ylabel('Loss function')\n",
    "    plt.xlabel('Number of epoche')\n",
    "    plt.show()\n",
    "#print(out_loss)\n",
    "\n",
    "#graph_loss_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(out): # точность классификации\n",
    "    acc = []\n",
    "    for i,net in enumerate(nets):\n",
    "        correct = 0\n",
    "        net.eval()\n",
    "        for x,y in test_loader:\n",
    "            if device == 'cuda':\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()     \n",
    "            out[i] = net(x)    \n",
    "            correct += out[i].argmax(1).eq(y).sum().cpu().numpy()\n",
    "        acc.append(correct / len(test_data))\n",
    "    print(sum(acc)/len(acc))   \n",
    "    return(acc)\n",
    "#test_acc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# коэффициенты информативности, см. статью practical variational inference\n",
    "# попробуем удалять параметры первого слоя по этому коэффициенту\n",
    "\n",
    "def init_coeff(prune_coef, mu, sigma):\n",
    "    for i,net in enumerate(nets): \n",
    "        mu.append(net[0].mean) \n",
    "        sigma.append(t.exp(2*net[0].log_sigma))\n",
    "        prune_coef.append((mu[i]**2/sigma[i]).cpu().detach().numpy())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем удалять по 10% от модели и смотреть качество\n",
    "def delete_10(acc_delete, prune_coef, mu, sigma, nets, out):\n",
    "    acc_delete = []\n",
    "    sorted_coefs = []\n",
    "    for i, net in enumerate(nets):\n",
    "        sorted_coefs.append(np.sort(prune_coef[i].flatten()))\n",
    "    for j in range(10):\n",
    "        for i,net in enumerate(nets): \n",
    "            ids = (prune_coef[i] <= sorted_coefs[i][round(j/10*len(sorted_coefs[i]))]) \n",
    "            net[0].mean.data*=(1-t.tensor(ids*1.0, device=device, dtype=t.float))\n",
    "            print ('nonzero params: ', (abs(net[0].mean)>0).float().mean())\n",
    "        acc_delete.append(test_acc(out))\n",
    "    return acc_delete    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(acc_delete, lamb):\n",
    "    proc = [0,10,20,30,40,50,60,70,80,90]\n",
    "    plt.rcParams['figure.figsize'] = 12, 12\n",
    "    for k, lam in enumerate(lamb):\n",
    "        acc_delete_n = np.array(acc_delete[k])\n",
    "        plt.plot(proc, np.mean(acc_delete_n, 1), label = 'lambda = {}'.format(str(lam)))\n",
    "        # откладываем ошибку вокруг среднего, альфа - прозрачность линии\n",
    "        plt.fill_between(proc, np.mean(acc_delete_n, 1)  + np.std(acc_delete_n, 1) , np.mean(acc_delete_n, 1) - np.std(acc_delete_n, 1) , alpha = 0.5 )\n",
    "    plt.ylabel('Точность классификации', fontsize = 20)\n",
    "    plt.xlabel('Процент удаления', fontsize = 20)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('Linear_1_sm3')\n",
    "    plt.show()\n",
    "\n",
    "#acc_delete = load('save_array_0.1')    \n",
    "#graph(acc_delete, lamb)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем, что фокусов тут нет, удаляем оставшиеся 10%\\\n",
    "def delete_last10():\n",
    "    flag = 0\n",
    "    for j in range(10):\n",
    "        for i,net in enumerate(nets): \n",
    "            if (flag == 0):\n",
    "                sorted_coefs = np.sort(prune_coef[i].flatten())\n",
    "                flag = 1\n",
    "            ids = (prune_coef[i] <= sorted_coefs[round((0.9+j/100)*len(sorted_coefs))]) \n",
    "            net[0].mean.data*=(1-t.tensor(ids*1.0, device=device, dtype=t.float))\n",
    "            print ('nonzero params: ', (abs(net[0].mean)>0).float().mean())\n",
    "        (test_acc())\n",
    "    for i,net in enumerate(nets):\n",
    "        net[0].mean.data*=0\n",
    "        print ('nonzero params: ', (abs(net[0].mean)>0).float().mean())\n",
    "    (test_acc())\n",
    "    \n",
    "#delete_last10()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:69: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:71: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of net: 0 tensor(500917.2500, device='cuda:0') tensor(59102.7148, device='cuda:0') tensor(441814.5312, device='cuda:0') 12.89463822153301\n",
      "Number of net: 0 tensor(1926606.7500, device='cuda:0') tensor(106888.0625, device='cuda:0') tensor(1819718.6250, device='cuda:0') 59.30711073278689\n",
      "Number of net: 0 tensor(369872.2188, device='cuda:0') tensor(36620.9922, device='cuda:0') tensor(333251.2188, device='cuda:0') 11.148246522463536\n",
      "Number of net: 0 tensor(1752076.2500, device='cuda:0') tensor(39810.8828, device='cuda:0') tensor(1712265.3750, device='cuda:0') 71.3160947476965\n",
      "Number of net: 0 tensor(125515.5781, device='cuda:0') tensor(71420.8359, device='cuda:0') tensor(54094.7461, device='cuda:0') 2.033441577886245\n",
      "Number of net: 0 tensor(1364090.2500, device='cuda:0') tensor(76970.9766, device='cuda:0') tensor(1287119.2500, device='cuda:0') 69.05615273325493\n",
      "Number of net: 0 tensor(450298.6875, device='cuda:0') tensor(45138.7539, device='cuda:0') tensor(405159.9375, device='cuda:0') 19.607675929978143\n",
      "Number of net: 0 tensor(271818.3750, device='cuda:0') tensor(84180.1719, device='cuda:0') tensor(187638.2188, device='cuda:0') 9.2995809785264\n",
      "Number of net: 0 tensor(172116., device='cuda:0') tensor(33042.3828, device='cuda:0') tensor(139073.6094, device='cuda:0') 7.196551707501539\n",
      "end of epoch:  0\n",
      "Number of net: 0\n",
      "tensor(16703.1562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Number of net: 0 tensor(644650.2500, device='cuda:0') tensor(34520.5391, device='cuda:0') tensor(610129.6875, device='cuda:0') 48.016947527826076\n",
      "Number of net: 0 tensor(75540.1719, device='cuda:0') tensor(70558.9375, device='cuda:0') tensor(4981.2324, device='cuda:0') 0.2854927696520565\n",
      "Number of net: 0 tensor(93385.7891, device='cuda:0') tensor(92625.1250, device='cuda:0') tensor(760.6672, device='cuda:0') 0.04591845651078295\n",
      "Number of net: 0 tensor(643827.6875, device='cuda:0') tensor(77052.0859, device='cuda:0') tensor(566775.6250, device='cuda:0') 69.4522992819025\n",
      "Number of net: 0 tensor(70423.7656, device='cuda:0') tensor(62954.6055, device='cuda:0') tensor(7469.1606, device='cuda:0') 0.5087716726892539\n",
      "Number of net: 0 tensor(233186.2812, device='cuda:0') tensor(80886.3750, device='cuda:0') tensor(152299.9062, device='cuda:0') 12.465183130842437\n",
      "Number of net: 0 tensor(616280.2500, device='cuda:0') tensor(81331.9766, device='cuda:0') tensor(534948.2500, device='cuda:0') 66.58875854905196\n",
      "Number of net: 0 tensor(65569.3438, device='cuda:0') tensor(63340.1094, device='cuda:0') tensor(2229.2319, device='cuda:0') 0.16976641495259218\n",
      "Number of net: 0 tensor(317128.3125, device='cuda:0') tensor(46716.8945, device='cuda:0') tensor(270411.4062, device='cuda:0') 34.62671430882244\n",
      "end of epoch:  1\n",
      "Number of net: 0\n",
      "tensor(37338.7188, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3e59d6ab71c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlam_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlog_lam_high\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnew_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m*=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnew_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mold_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnew_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m*=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mnew_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mold_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "loss_fn_nets =[]\n",
    "nets = []\n",
    "optimizer_nets = []\n",
    "mu_glob = []\n",
    "sigma_glob = []\n",
    "prune_coef_glob = []\n",
    "init_nets_output =  init_nets(loss_fn_nets)\n",
    "train_nets(init_nets_output[0], init_nets_output[1], init_nets_output[2], init_nets_output[3], init_nets_output[4])\n",
    "old_nets = nets[:]\n",
    "\n",
    "for k,lam in enumerate(lamb):\n",
    "    for i in enumerate(nets):\n",
    "        new_net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "        for j in range(0, 2): # бежим по слоям\n",
    "            lam_param = lam / 10**log_lam_high\n",
    "            new_net[j].mean.data*=0\n",
    "            new_net[j].mean.data+=old_nets[i][j].mean(lam_param)\n",
    "            new_net[j].mean_b.data*=0\n",
    "            new_net[j].mean_b.data+=old_nets[i][j].mean_b(lam_param)\n",
    "            new_net[j].log_sigma.data*=0\n",
    "            new_net[j].log_sigma.data+=old_nets[i][j].log_sigma(lam_param)\n",
    "            new_net[j].log_sigma_b.data*=0\n",
    "            new_net[j].log_sigma_b.data+=old_nets[i][j].log_sigma_b(lam_param)\n",
    "            \n",
    "    nets[i] = new_net        \n",
    "    acc_delete.append(None)\n",
    "    init_coeff(prune_coef_glob, mu_glob, sigma_glob)\n",
    "    acc_delete[k]= delete_10(acc_delete[k], prune_coef_glob, mu_glob, sigma_glob, nets, init_nets_output[0])\n",
    "\n",
    "    \n",
    "init_coeff(prune_coef_glob, mu_glob, sigma_glob)    \n",
    "#graph_loss_func()\n",
    "graph(acc_delete,lamb)\n",
    "save(acc_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legin/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "new_net = VarSeqNet(VarLayer(784,  hidden_num), VarLayer(hidden_num, 10, act=lambda x:x))\n",
    "i = 0 \n",
    "for j in range(0, 2): # бежим по слоям\n",
    "    new_net[j].mean.data*=0\n",
    "    new_net[j].mean.data+=old_nets[i][j].mean(lam)\n",
    "    new_net[j].mean_b.data*=0\n",
    "    new_net[j].mean_b.data+=old_nets[i][j].mean_b(lam)\n",
    "    new_net[j].log_sigma.data*=0\n",
    "    new_net[j].log_sigma.data+=old_nets[i][j].log_sigma(lam)\n",
    "    new_net[j].log_sigma_b.data*=0\n",
    "    new_net[j].log_sigma_b.data+=old_nets[i][j].log_sigma_b(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0, VarSeqNet(\n",
    "  (0): VarLayerLinearAppr(\n",
    "    (mean): LinearApprNet()\n",
    "    (log_sigma): LinearApprNet()\n",
    "    (mean_b): LinearApprNet()\n",
    "    (log_sigma_b): LinearApprNet()\n",
    "  )\n",
    "  (1): VarLayerLinearAppr(\n",
    "    (mean): LinearApprNet()\n",
    "    (log_sigma): LinearApprNet()\n",
    "    (mean_b): LinearApprNet()\n",
    "    (log_sigma_b): LinearApprNet()\n",
    "  )\n",
    ")) 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1213.8149,  885.8934, 1215.5417,  883.6002,  768.9736, 1339.3053,\n",
       "         1235.9641, 1001.3223, 1015.4268, 1144.4662],\n",
       "        [1113.5548,  825.9999, 1122.1970,  829.0692,  703.5684, 1222.4860,\n",
       "         1139.6187,  909.0403,  941.6868, 1039.6541],\n",
       "        [1339.7222, 1000.9510, 1352.3000,  987.5376,  854.2031, 1484.1738,\n",
       "         1375.6842, 1108.0099, 1130.4620, 1266.8187],\n",
       "        [1002.4268,  716.7179,  990.4283,  723.5088,  620.0521, 1093.7192,\n",
       "         1012.1857,  808.2672,  829.2755,  925.5312],\n",
       "        [1214.7711,  881.5259, 1217.4650,  878.8027,  773.8610, 1341.7554,\n",
       "         1239.9010,  995.3021, 1017.7711, 1147.7567],\n",
       "        [1264.0046,  948.8234, 1279.3776,  936.2783,  807.3743, 1401.2827,\n",
       "         1299.3010, 1048.4663, 1071.0332, 1198.0283],\n",
       "        [1162.2483,  848.7446, 1162.3492,  849.2804,  742.9947, 1287.5316,\n",
       "         1187.1021,  956.5825,  976.6001, 1098.8002],\n",
       "        [1161.8960,  854.9814, 1167.5698,  849.1383,  741.3011, 1291.7340,\n",
       "         1193.3240,  957.9014,  978.7094, 1101.9408],\n",
       "        [1055.0309,  765.3071, 1059.2241,  759.6130,  665.5242, 1162.6490,\n",
       "         1075.7008,  858.5210,  879.9997,  991.1271],\n",
       "        [1004.6137,  726.8611, 1002.0803,  724.5376,  640.0945, 1113.4681,\n",
       "         1027.1365,  826.8737,  842.0551,  953.8392],\n",
       "        [1106.1322,  800.0879, 1096.8038,  803.9124,  687.3207, 1209.8391,\n",
       "         1110.4406,  893.7808,  922.4234, 1020.5682],\n",
       "        [1102.7822,  812.9637, 1107.4323,  809.6740,  701.6365, 1216.2111,\n",
       "         1128.6803,  904.2400,  930.2513, 1036.0732],\n",
       "        [1142.2748,  827.7216, 1142.2802,  825.7719,  727.3287, 1263.5828,\n",
       "         1167.0024,  938.4810,  956.5614, 1082.3589],\n",
       "        [1114.2269,  798.5081, 1101.5996,  802.3863,  690.2389, 1217.2181,\n",
       "         1117.6390,  899.4980,  925.4005, 1032.5500],\n",
       "        [1233.8114,  925.1736, 1247.6390,  913.9560,  785.8114, 1366.6178,\n",
       "         1268.7147, 1020.9987, 1044.6176, 1168.4497],\n",
       "        [1183.5101,  869.5018, 1183.6874,  868.3387,  748.7723, 1305.7465,\n",
       "         1203.5662,  968.9479,  995.5602, 1110.4602],\n",
       "        [1151.9600,  832.7865, 1152.1295,  831.0288,  731.0743, 1272.2629,\n",
       "         1175.2589,  943.7976,  962.8796, 1090.2181],\n",
       "        [1156.8367,  843.4811, 1156.2766,  844.3018,  730.8408, 1274.0698,\n",
       "         1175.9003,  952.3433,  965.7720, 1087.3026],\n",
       "        [ 959.4685,  712.2230,  965.9986,  715.6331,  614.0234, 1060.5006,\n",
       "          986.8311,  790.3065,  813.6065,  907.2649],\n",
       "        [1238.1980,  900.6582, 1239.1887,  898.3592,  790.9783, 1372.5751,\n",
       "         1266.3883, 1016.5413, 1036.9979, 1170.7600],\n",
       "        [1055.3127,  769.6971, 1053.7079,  772.3586,  671.9940, 1168.5079,\n",
       "         1077.4681,  872.6092,  886.8295, 1000.7932],\n",
       "        [1187.3479,  870.8310, 1190.4342,  873.1470,  758.6207, 1314.7000,\n",
       "         1221.4469,  968.9639, 1000.5348, 1116.7037],\n",
       "        [1133.0822,  839.0484, 1142.8619,  830.1492,  727.1671, 1254.1469,\n",
       "         1167.9001,  936.0645,  955.9817, 1072.5751],\n",
       "        [1206.0485,  878.1080, 1204.0670,  879.5761,  762.6722, 1333.1544,\n",
       "         1228.8014,  983.7407, 1009.8130, 1130.5684],\n",
       "        [1291.6489,  943.6645, 1295.6732,  938.1758,  823.0602, 1430.2521,\n",
       "         1321.5273, 1063.3856, 1081.4053, 1221.7062],\n",
       "        [ 775.9562,  540.8037,  758.5713,  550.2057,  476.9268,  839.6254,\n",
       "          774.0443,  616.4178,  637.5400,  703.3408],\n",
       "        [1262.4897,  919.6780, 1261.5057,  916.8309,  797.2607, 1393.0698,\n",
       "         1283.7463, 1038.5009, 1051.4291, 1189.8137],\n",
       "        [1162.3121,  842.5047, 1163.0836,  842.0964,  742.7407, 1286.8655,\n",
       "         1187.2418,  951.1564,  975.0425, 1097.7513],\n",
       "        [1033.9384,  742.7003, 1023.7916,  753.6508,  639.1855, 1126.3632,\n",
       "         1036.4337,  831.7239,  860.6248,  953.3066],\n",
       "        [1303.6235,  970.5648, 1311.9340,  960.6790,  829.9347, 1443.7478,\n",
       "         1336.0490, 1074.9747, 1099.9890, 1230.5431],\n",
       "        [1112.7043,  821.3407, 1112.8785,  825.1525,  704.0001, 1231.5591,\n",
       "         1137.2792,  914.7655,  935.8028, 1048.7450],\n",
       "        [1329.9685,  985.8568, 1335.3694,  975.9597,  844.0504, 1471.8356,\n",
       "         1360.0922, 1096.3303, 1117.1455, 1254.8982],\n",
       "        [1094.7808,  808.6679, 1095.6768,  812.6392,  695.2275, 1212.7867,\n",
       "         1118.1213,  895.7611,  924.9702, 1029.1608],\n",
       "        [1181.4437,  853.0333, 1174.9203,  847.8520,  744.7269, 1297.0996,\n",
       "         1200.7886,  960.3172,  983.4108, 1101.1309],\n",
       "        [1089.5023,  796.8277, 1096.9160,  793.4803,  689.8644, 1204.1743,\n",
       "         1108.9688,  897.8516,  915.6964, 1030.6798],\n",
       "        [1092.6499,  802.0144, 1104.6135,  800.3990,  690.2056, 1204.0435,\n",
       "         1115.1886,  892.2275,  919.6136, 1022.9302],\n",
       "        [1171.6873,  852.2813, 1174.0078,  851.6686,  737.9956, 1292.9839,\n",
       "         1192.7706,  964.4850,  976.4904, 1104.4377],\n",
       "        [1279.8199,  955.3799, 1289.7539,  942.8917,  814.4152, 1417.7841,\n",
       "         1312.1676, 1057.3673, 1080.2207, 1209.7961],\n",
       "        [1245.4636,  924.5058, 1254.9376,  922.3326,  785.9587, 1372.2797,\n",
       "         1271.9065, 1019.3018, 1050.3654, 1166.7529],\n",
       "        [1205.7753,  906.9663, 1221.0624,  896.1619,  769.2404, 1335.8564,\n",
       "         1240.4474,  997.6205, 1024.6687, 1143.0664],\n",
       "        [1394.8994, 1035.0139, 1403.2163, 1022.6022,  885.2772, 1543.6072,\n",
       "         1428.2947, 1149.7947, 1171.8206, 1316.7170],\n",
       "        [1234.2463,  900.1197, 1237.5350,  896.0052,  780.3107, 1364.4656,\n",
       "         1259.8972, 1017.8138, 1030.8387, 1165.6505],\n",
       "        [1088.5558,  799.7700, 1097.3922,  795.2089,  701.5529, 1210.6924,\n",
       "         1118.9413,  901.7348,  919.4437, 1038.4702],\n",
       "        [1260.8257,  939.2440, 1276.6754,  929.8670,  806.9221, 1393.5586,\n",
       "         1294.6140, 1039.9116, 1068.7317, 1193.2734],\n",
       "        [1215.5618,  899.1376, 1219.9398,  899.6274,  772.4246, 1344.3591,\n",
       "         1246.4971,  998.8228, 1023.1666, 1143.8209],\n",
       "        [1214.5325,  888.2198, 1212.3229,  893.5019,  767.9814, 1340.2347,\n",
       "         1235.2024,  989.7374, 1019.8769, 1135.8445],\n",
       "        [1194.7740,  888.1383, 1203.9762,  882.1531,  761.4463, 1325.8440,\n",
       "         1225.4667,  985.2586, 1010.0103, 1129.3053],\n",
       "        [1256.6650,  925.7350, 1270.0209,  918.4496,  801.4917, 1391.3363,\n",
       "         1290.8925, 1034.4359, 1057.5919, 1188.2239],\n",
       "        [ 882.3761,  634.4458,  879.8942,  641.2602,  568.7238,  982.0277,\n",
       "          902.1945,  722.9243,  745.1243,  837.5640],\n",
       "        [1119.4977,  819.4255, 1128.1768,  816.5215,  720.9461, 1240.3503,\n",
       "         1151.3230,  923.1666,  944.4438, 1063.2751],\n",
       "        [1230.1693,  900.2593, 1232.3743,  902.9976,  781.8223, 1357.0557,\n",
       "         1264.3557, 1005.2056, 1031.3872, 1154.1401],\n",
       "        [ 923.4564,  678.5153,  926.4617,  686.3658,  583.9376, 1020.4092,\n",
       "          943.3774,  757.4249,  778.7120,  870.8770],\n",
       "        [1148.5693,  830.2279, 1140.0096,  829.3983,  723.0687, 1268.8618,\n",
       "         1165.0298,  935.1100,  954.9310, 1073.4139],\n",
       "        [1265.6339,  929.6724, 1266.3108,  928.9948,  802.3307, 1399.1946,\n",
       "         1290.1858, 1036.9296, 1062.9387, 1189.9510],\n",
       "        [ 984.7880,  726.6226,  992.9525,  723.3234,  632.6845, 1087.6653,\n",
       "         1014.3206,  811.7478,  833.4928,  930.4764],\n",
       "        [1136.6107,  821.8636, 1129.2489,  830.7013,  713.0267, 1245.5730,\n",
       "         1148.6978,  919.4561,  953.3442, 1054.4016],\n",
       "        [1065.7957,  772.9418, 1067.4376,  772.8953,  684.2037, 1180.3480,\n",
       "         1092.5101,  872.3882,  895.6629, 1007.6088],\n",
       "        [1328.8558,  992.5547, 1341.7382,  979.4781,  844.9699, 1471.5966,\n",
       "         1362.9536, 1098.3242, 1121.3163, 1257.1777],\n",
       "        [1088.2820,  785.3709, 1084.9475,  783.9604,  691.8876, 1202.8215,\n",
       "         1110.1906,  892.6973,  908.5729, 1030.7645],\n",
       "        [1330.9313,  977.9062, 1329.9781,  970.1743,  843.6094, 1469.2367,\n",
       "         1356.8473, 1092.7114, 1113.6824, 1248.8262],\n",
       "        [1101.1812,  809.7220, 1105.1852,  809.8719,  699.6218, 1218.2960,\n",
       "         1125.3260,  912.6066,  923.8100, 1040.3481],\n",
       "        [1035.4622,  753.7729, 1041.9268,  751.0491,  654.8101, 1138.8579,\n",
       "         1056.0887,  844.3660,  872.7928,  973.6345],\n",
       "        [1292.9025,  942.0190, 1294.7795,  936.4296,  819.6076, 1428.9954,\n",
       "         1319.5942, 1059.7574, 1081.0751, 1219.5033],\n",
       "        [1163.7986,  859.3628, 1176.5050,  859.6938,  741.1757, 1287.1127,\n",
       "         1192.1193,  956.1595,  983.4633, 1099.9132]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
